/**\n * YouTube Automation Platform - Experiment Manager\n * \n * Enhanced experiment management system that extends ABTestManager with\n * advanced experiment lifecycle management, automated decision making,\n * and integration with feature flags and configuration systems.\n * \n * Features:\n * - Advanced experiment lifecycle management\n * - Automated experiment decisions based on statistical significance\n * - Integration with FeatureFlagManager for feature rollouts\n * - Multi-metric optimization and analysis\n * - Experiment scheduling and automation\n * - Real-time monitoring and alerting\n * \n * @fileoverview Provides comprehensive experiment management\n * @author YouTube Automation Platform Team\n * @version 2.1.0\n * @since 2025-10-06\n */\n\nconst ABTestManager = require('./ABTestManager');\nconst ConfigurationManager = require('./ConfigurationManager');\n\nclass ExperimentManager {\n    constructor(options = {}) {\n        this.region = options.region || process.env.AWS_REGION || 'us-east-1';\n        this.environment = options.environment || process.env.ENVIRONMENT || 'production';\n        \n        // Initialize configuration manager\n        this.configManager = options.configManager || new ConfigurationManager({\n            region: this.region,\n            environment: this.environment\n        });\n        \n        // Initialize A/B test manager\n        this.abTestManager = options.abTestManager || new ABTestManager({\n            region: this.region,\n            environment: this.environment,\n            configManager: this.configManager\n        });\n        \n        // Experiment monitoring\n        this.activeExperiments = new Map();\n        this.experimentScheduler = new Map();\n        this.monitoringInterval = options.monitoringInterval || 3600000; // 1 hour\n        \n        // Decision thresholds\n        this.autoDecisionThresholds = {\n            minimumRuntime: options.minimumRuntime || 7 * 24 * 60 * 60 * 1000, // 7 days\n            maximumRuntime: options.maximumRuntime || 30 * 24 * 60 * 60 * 1000, // 30 days\n            significanceThreshold: options.significanceThreshold || 0.05,\n            minimumEffect: options.minimumEffect || 0.05, // 5% minimum effect\n            confidenceThreshold: options.confidenceThreshold || 0.95\n        };\n        \n        // Start monitoring\n        this.startExperimentMonitoring();\n        \n        console.log(`ExperimentManager initialized for environment: ${this.environment}`);\n    }\n\n    /**\n     * Create and configure a comprehensive experiment\n     * \n     * @param {Object} experimentConfig - Enhanced experiment configuration\n     * @returns {Promise<Object>} Created experiment\n     */\n    async createComprehensiveExperiment(experimentConfig) {\n        try {\n            // Enhance configuration with advanced settings\n            const enhancedConfig = {\n                ...experimentConfig,\n                \n                // Advanced experiment settings\n                autoDecision: experimentConfig.autoDecision !== false,\n                multiMetric: experimentConfig.multiMetric || false,\n                \n                // Traffic allocation\n                trafficAllocation: experimentConfig.trafficAllocation || 100, // Percentage of traffic\n                \n                // Success criteria\n                successCriteria: experimentConfig.successCriteria || {\n                    primaryMetric: 'conversion_rate',\n                    minimumEffect: this.autoDecisionThresholds.minimumEffect,\n                    confidenceLevel: this.autoDecisionThresholds.confidenceThreshold\n                },\n                \n                // Monitoring settings\n                monitoring: {\n                    enabled: experimentConfig.monitoring?.enabled !== false,\n                    alertThresholds: experimentConfig.monitoring?.alertThresholds || {\n                        significantDrop: -0.1, // 10% drop triggers alert\n                        significantIncrease: 0.2, // 20% increase triggers alert\n                        lowTraffic: 0.1 // Less than 10% expected traffic\n                    },\n                    checkInterval: experimentConfig.monitoring?.checkInterval || 3600000 // 1 hour\n                },\n                \n                // Integration settings\n                integration: {\n                    featureFlag: experimentConfig.integration?.featureFlag || null,\n                    configurationKey: experimentConfig.integration?.configurationKey || null,\n                    rolloutStrategy: experimentConfig.integration?.rolloutStrategy || 'gradual'\n                }\n            };\n            \n            // Create base experiment\n            const experiment = await this.abTestManager.createExperiment(enhancedConfig);\n            \n            // Add to active experiments tracking\n            this.activeExperiments.set(experiment.id, {\n                experiment,\n                config: enhancedConfig,\n                startedAt: null,\n                lastChecked: null,\n                alerts: [],\n                decisions: []\n            });\n            \n            console.log(`Created comprehensive experiment: ${experiment.id}`);\n            return experiment;\n            \n        } catch (error) {\n            console.error('Failed to create comprehensive experiment:', error);\n            throw error;\n        }\n    }\n\n    /**\n     * Start experiment with enhanced monitoring\n     * \n     * @param {string} experimentId - Experiment ID\n     * @returns {Promise<Object>} Started experiment\n     */\n    async startExperimentWithMonitoring(experimentId) {\n        try {\n            // Start base experiment\n            const experiment = await this.abTestManager.startExperiment(experimentId);\n            \n            // Update tracking\n            const tracking = this.activeExperiments.get(experimentId);\n            if (tracking) {\n                tracking.startedAt = new Date();\n                tracking.lastChecked = new Date();\n            }\n            \n            // Schedule monitoring\n            this.scheduleExperimentCheck(experimentId);\n            \n            console.log(`Started experiment with monitoring: ${experimentId}`);\n            return experiment;\n            \n        } catch (error) {\n            console.error(`Failed to start experiment with monitoring ${experimentId}:`, error);\n            throw error;\n        }\n    }\n\n    /**\n     * Perform automated experiment analysis and decision making\n     * \n     * @param {string} experimentId - Experiment ID\n     * @returns {Promise<Object>} Analysis and decision\n     */\n    async performAutomatedAnalysis(experimentId) {\n        try {\n            const tracking = this.activeExperiments.get(experimentId);\n            if (!tracking) {\n                throw new Error(`Experiment ${experimentId} not found in active experiments`);\n            }\n            \n            const { experiment, config } = tracking;\n            \n            // Get current results\n            const results = await this.abTestManager.getExperimentResults(experimentId);\n            \n            // Perform comprehensive analysis\n            const analysis = {\n                experimentId,\n                timestamp: new Date().toISOString(),\n                runtime: Date.now() - new Date(experiment.actualStartDate || experiment.startDate).getTime(),\n                \n                // Statistical analysis\n                statistical: results.statisticalAnalysis,\n                \n                // Performance analysis\n                performance: await this.analyzePerformance(results),\n                \n                // Risk analysis\n                risk: await this.analyzeRisk(results, config),\n                \n                // Decision recommendation\n                decision: null\n            };\n            \n            // Make automated decision if enabled\n            if (config.autoDecision) {\n                analysis.decision = await this.makeAutomatedDecision(analysis, config);\n            }\n            \n            // Store analysis\n            tracking.decisions.push(analysis);\n            \n            return analysis;\n            \n        } catch (error) {\n            console.error(`Failed to perform automated analysis for ${experimentId}:`, error);\n            throw error;\n        }\n    }\n\n    /**\n     * Make automated experiment decision\n     * \n     * @param {Object} analysis - Experiment analysis\n     * @param {Object} config - Experiment configuration\n     * @returns {Promise<Object>} Automated decision\n     */\n    async makeAutomatedDecision(analysis, config) {\n        const decision = {\n            action: 'continue',\n            confidence: 0,\n            reasoning: [],\n            implementation: null,\n            timestamp: new Date().toISOString()\n        };\n        \n        const { runtime } = analysis;\n        const { minimumRuntime, maximumRuntime, significanceThreshold, minimumEffect } = this.autoDecisionThresholds;\n        \n        // Check minimum runtime\n        if (runtime < minimumRuntime) {\n            decision.reasoning.push('Experiment has not reached minimum runtime');\n            return decision;\n        }\n        \n        // Check for significant results\n        const hasSignificantResult = Object.values(analysis.statistical)\n            .some(result => result.significant === true && Math.abs(result.effect) >= minimumEffect);\n        \n        if (hasSignificantResult) {\n            // Find winning variant\n            const winner = this.identifyWinningVariant(analysis);\n            \n            if (winner) {\n                decision.action = 'implement';\n                decision.confidence = 0.95;\n                decision.reasoning.push(`Statistically significant improvement detected: ${winner.variant}`);\n                decision.implementation = {\n                    variant: winner.variant,\n                    effect: winner.effect,\n                    rolloutStrategy: config.integration?.rolloutStrategy || 'gradual'\n                };\n                \n                // Auto-implement if configured\n                if (config.integration?.featureFlag) {\n                    await this.implementWinningVariant(analysis.experimentId, winner, config);\n                }\n            }\n        } else if (runtime > maximumRuntime) {\n            // Stop experiment if maximum runtime exceeded\n            decision.action = 'stop';\n            decision.confidence = 0.8;\n            decision.reasoning.push('Maximum runtime exceeded without significant results');\n        } else if (analysis.risk.highRisk) {\n            // Stop if high risk detected\n            decision.action = 'stop';\n            decision.confidence = 0.9;\n            decision.reasoning.push('High risk detected: ' + analysis.risk.reasons.join(', '));\n        }\n        \n        return decision;\n    }\n\n    /**\n     * Implement winning variant automatically\n     * \n     * @param {string} experimentId - Experiment ID\n     * @param {Object} winner - Winning variant information\n     * @param {Object} config - Experiment configuration\n     */\n    async implementWinningVariant(experimentId, winner, config) {\n        try {\n            if (config.integration?.featureFlag) {\n                // Update feature flag to use winning variant\n                const FeatureFlagManager = require('./FeatureFlagManager');\n                const flagManager = new FeatureFlagManager({\n                    environment: this.environment,\n                    configManager: this.configManager\n                });\n                \n                await flagManager.createFlag(config.integration.featureFlag, {\n                    enabled: true,\n                    rolloutPercentage: 100,\n                    value: winner.variant,\n                    metadata: {\n                        description: `Auto-implemented from experiment ${experimentId}`,\n                        experimentId,\n                        implementedAt: new Date().toISOString(),\n                        effect: winner.effect\n                    }\n                });\n                \n                console.log(`Auto-implemented winning variant ${winner.variant} for experiment ${experimentId}`);\n            }\n            \n            if (config.integration?.configurationKey) {\n                // Update configuration with winning variant\n                await this.configManager.set(config.integration.configurationKey, winner.variant);\n                console.log(`Updated configuration ${config.integration.configurationKey} with winning variant`);\n            }\n            \n        } catch (error) {\n            console.error(`Failed to implement winning variant for ${experimentId}:`, error);\n        }\n    }\n\n    /**\n     * Analyze experiment performance\n     * \n     * @param {Object} results - Experiment results\n     * @returns {Promise<Object>} Performance analysis\n     */\n    async analyzePerformance(results) {\n        const performance = {\n            overallHealth: 'good',\n            trafficDistribution: {},\n            conversionTrends: {},\n            anomalies: []\n        };\n        \n        // Analyze traffic distribution\n        const variants = Object.keys(results.metrics?.variants || {});\n        const totalUsers = Object.values(results.metrics?.variants || {})\n            .reduce((sum, variant) => sum + (variant.totalUsers || 0), 0);\n        \n        variants.forEach(variant => {\n            const variantData = results.metrics.variants[variant];\n            const expectedTraffic = 1 / variants.length; // Assuming equal split\n            const actualTraffic = (variantData.totalUsers || 0) / totalUsers;\n            \n            performance.trafficDistribution[variant] = {\n                expected: expectedTraffic,\n                actual: actualTraffic,\n                deviation: Math.abs(actualTraffic - expectedTraffic)\n            };\n            \n            // Check for traffic anomalies\n            if (Math.abs(actualTraffic - expectedTraffic) > 0.1) {\n                performance.anomalies.push({\n                    type: 'traffic_imbalance',\n                    variant,\n                    severity: 'medium',\n                    description: `Traffic deviation of ${(Math.abs(actualTraffic - expectedTraffic) * 100).toFixed(1)}%`\n                });\n            }\n        });\n        \n        return performance;\n    }\n\n    /**\n     * Analyze experiment risk\n     * \n     * @param {Object} results - Experiment results\n     * @param {Object} config - Experiment configuration\n     * @returns {Promise<Object>} Risk analysis\n     */\n    async analyzeRisk(results, config) {\n        const risk = {\n            level: 'low',\n            highRisk: false,\n            reasons: [],\n            mitigations: []\n        };\n        \n        // Check for significant negative effects\n        const hasNegativeEffect = Object.values(results.statisticalAnalysis || {})\n            .some(result => result.significant && result.effect < -0.1); // 10% drop\n        \n        if (hasNegativeEffect) {\n            risk.level = 'high';\n            risk.highRisk = true;\n            risk.reasons.push('Significant negative effect detected');\n            risk.mitigations.push('Consider stopping experiment immediately');\n        }\n        \n        // Check for low sample sizes\n        const variants = Object.values(results.metrics?.variants || {});\n        const hasLowSampleSize = variants.some(variant => (variant.totalUsers || 0) < 30);\n        \n        if (hasLowSampleSize) {\n            risk.level = risk.level === 'high' ? 'high' : 'medium';\n            risk.reasons.push('Low sample size in one or more variants');\n            risk.mitigations.push('Increase traffic allocation or extend experiment duration');\n        }\n        \n        return risk;\n    }\n\n    /**\n     * Identify winning variant from analysis\n     * \n     * @param {Object} analysis - Experiment analysis\n     * @returns {Object|null} Winning variant information\n     */\n    identifyWinningVariant(analysis) {\n        let bestVariant = null;\n        let bestEffect = 0;\n        \n        Object.entries(analysis.statistical).forEach(([comparison, result]) => {\n            if (result.significant && result.effect > bestEffect) {\n                // Extract variant name from comparison (e.g., \"control_vs_variant_a\")\n                const variants = comparison.split('_vs_');\n                const winningVariant = result.effect > 0 ? variants[1] : variants[0];\n                \n                bestVariant = {\n                    variant: winningVariant,\n                    effect: Math.abs(result.effect),\n                    confidence: 1 - result.pValue,\n                    comparison\n                };\n                bestEffect = Math.abs(result.effect);\n            }\n        });\n        \n        return bestVariant;\n    }\n\n    /**\n     * Start experiment monitoring\n     */\n    startExperimentMonitoring() {\n        setInterval(async () => {\n            try {\n                await this.checkAllActiveExperiments();\n            } catch (error) {\n                console.error('Error in experiment monitoring:', error);\n            }\n        }, this.monitoringInterval);\n        \n        console.log('Started experiment monitoring');\n    }\n\n    /**\n     * Check all active experiments\n     */\n    async checkAllActiveExperiments() {\n        const activeExperimentIds = Array.from(this.activeExperiments.keys());\n        \n        for (const experimentId of activeExperimentIds) {\n            try {\n                await this.checkExperiment(experimentId);\n            } catch (error) {\n                console.error(`Error checking experiment ${experimentId}:`, error);\n            }\n        }\n    }\n\n    /**\n     * Check individual experiment\n     * \n     * @param {string} experimentId - Experiment ID\n     */\n    async checkExperiment(experimentId) {\n        const tracking = this.activeExperiments.get(experimentId);\n        if (!tracking) return;\n        \n        const { config } = tracking;\n        \n        // Perform automated analysis\n        const analysis = await this.performAutomatedAnalysis(experimentId);\n        \n        // Check for alerts\n        await this.checkForAlerts(experimentId, analysis);\n        \n        // Execute automated decisions\n        if (analysis.decision && analysis.decision.action !== 'continue') {\n            await this.executeAutomatedDecision(experimentId, analysis.decision);\n        }\n        \n        // Update last checked time\n        tracking.lastChecked = new Date();\n    }\n\n    /**\n     * Check for experiment alerts\n     * \n     * @param {string} experimentId - Experiment ID\n     * @param {Object} analysis - Experiment analysis\n     */\n    async checkForAlerts(experimentId, analysis) {\n        const tracking = this.activeExperiments.get(experimentId);\n        if (!tracking) return;\n        \n        const { config } = tracking;\n        const alertThresholds = config.monitoring?.alertThresholds || {};\n        \n        // Check for significant drops\n        Object.entries(analysis.statistical).forEach(([comparison, result]) => {\n            if (result.significant && result.effect < alertThresholds.significantDrop) {\n                this.triggerAlert(experimentId, {\n                    type: 'significant_drop',\n                    severity: 'high',\n                    message: `Significant drop detected in ${comparison}: ${(result.effect * 100).toFixed(1)}%`,\n                    data: result\n                });\n            }\n        });\n        \n        // Check for traffic issues\n        if (analysis.performance?.anomalies?.length > 0) {\n            analysis.performance.anomalies.forEach(anomaly => {\n                this.triggerAlert(experimentId, {\n                    type: anomaly.type,\n                    severity: anomaly.severity,\n                    message: anomaly.description,\n                    data: anomaly\n                });\n            });\n        }\n    }\n\n    /**\n     * Trigger experiment alert\n     * \n     * @param {string} experimentId - Experiment ID\n     * @param {Object} alert - Alert information\n     */\n    triggerAlert(experimentId, alert) {\n        const tracking = this.activeExperiments.get(experimentId);\n        if (!tracking) return;\n        \n        const alertWithTimestamp = {\n            ...alert,\n            experimentId,\n            timestamp: new Date().toISOString()\n        };\n        \n        tracking.alerts.push(alertWithTimestamp);\n        \n        console.warn(`Experiment alert [${experimentId}]: ${alert.message}`);\n        \n        // In production, this would send notifications via SNS, email, etc.\n    }\n\n    /**\n     * Execute automated decision\n     * \n     * @param {string} experimentId - Experiment ID\n     * @param {Object} decision - Automated decision\n     */\n    async executeAutomatedDecision(experimentId, decision) {\n        try {\n            switch (decision.action) {\n                case 'implement':\n                    await this.abTestManager.stopExperiment(experimentId, 'auto_implement_winner');\n                    console.log(`Auto-implemented winning variant for experiment ${experimentId}`);\n                    break;\n                    \n                case 'stop':\n                    await this.abTestManager.stopExperiment(experimentId, 'auto_stop_no_significance');\n                    console.log(`Auto-stopped experiment ${experimentId}`);\n                    break;\n            }\n            \n            // Remove from active experiments\n            this.activeExperiments.delete(experimentId);\n            \n        } catch (error) {\n            console.error(`Failed to execute automated decision for ${experimentId}:`, error);\n        }\n    }\n\n    /**\n     * Schedule experiment check\n     * \n     * @param {string} experimentId - Experiment ID\n     */\n    scheduleExperimentCheck(experimentId) {\n        const tracking = this.activeExperiments.get(experimentId);\n        if (!tracking) return;\n        \n        const checkInterval = tracking.config.monitoring?.checkInterval || this.monitoringInterval;\n        \n        const intervalId = setInterval(async () => {\n            try {\n                await this.checkExperiment(experimentId);\n            } catch (error) {\n                console.error(`Error in scheduled check for experiment ${experimentId}:`, error);\n            }\n        }, checkInterval);\n        \n        this.experimentScheduler.set(experimentId, intervalId);\n    }\n\n    /**\n     * Get experiment dashboard data\n     * \n     * @returns {Promise<Object>} Dashboard data\n     */\n    async getExperimentDashboard() {\n        const dashboard = {\n            summary: {\n                totalActive: this.activeExperiments.size,\n                totalAlerts: 0,\n                recentDecisions: 0\n            },\n            experiments: [],\n            alerts: [],\n            recentDecisions: []\n        };\n        \n        // Collect data from active experiments\n        for (const [experimentId, tracking] of this.activeExperiments) {\n            const results = await this.abTestManager.getExperimentResults(experimentId);\n            \n            dashboard.experiments.push({\n                id: experimentId,\n                name: tracking.experiment.name,\n                status: tracking.experiment.status,\n                runtime: tracking.startedAt ? Date.now() - tracking.startedAt.getTime() : 0,\n                alerts: tracking.alerts.length,\n                lastDecision: tracking.decisions[tracking.decisions.length - 1],\n                performance: results.statisticalAnalysis\n            });\n            \n            dashboard.summary.totalAlerts += tracking.alerts.length;\n            dashboard.alerts.push(...tracking.alerts.slice(-5)); // Last 5 alerts\n            dashboard.recentDecisions.push(...tracking.decisions.slice(-3)); // Last 3 decisions\n        }\n        \n        dashboard.summary.recentDecisions = dashboard.recentDecisions.length;\n        \n        return dashboard;\n    }\n\n    /**\n     * Clean up completed experiments\n     */\n    async cleanupCompletedExperiments() {\n        const completedExperiments = [];\n        \n        for (const [experimentId, tracking] of this.activeExperiments) {\n            if (tracking.experiment.status === 'completed') {\n                completedExperiments.push(experimentId);\n            }\n        }\n        \n        completedExperiments.forEach(experimentId => {\n            // Clear scheduled checks\n            const intervalId = this.experimentScheduler.get(experimentId);\n            if (intervalId) {\n                clearInterval(intervalId);\n                this.experimentScheduler.delete(experimentId);\n            }\n            \n            // Remove from active tracking\n            this.activeExperiments.delete(experimentId);\n        });\n        \n        console.log(`Cleaned up ${completedExperiments.length} completed experiments`);\n    }\n}\n\nmodule.exports = ExperimentManager;"